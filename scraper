#!/usr/bin/env python
#
# Command line web page fetcher for use in dodgy site analysis.
# Can log redirects, use a proxy server, display headers and cookies received as well as output the webpage raw.
# Features to be added, identify interesting content in the page such as JavaScript, Flash or Java
#
# This is version 2, rewritten to use Mechanize so you'll need that to run it as well as argparse, everything else
# is pretty standard.
#
# Author: Marc Wickenden
# Date: January 2011

import sys
import mechanize
import logging
import argparse
import cookielib

# Now parse the command line arguments
parser = argparse.ArgumentParser(description='Command line web page fetcher', epilog="Have fun!")
parser.add_argument('--target', '-t', action="store", dest="url", help='The target url to open', required=True)
parser.add_argument('--cookies', '-c', action='store_true', dest="cookies", help='Display received cookies')
parser.add_argument('--forms', '-f', action='store_true', dest="forms", help='Display forms in page')
parser.add_argument('--submit-form', action='store', dest="submitform", type=int, help='Submit this form number or name')
parser.add_argument('--headers', '-x', action='store_true', dest="headers", help='Print the HTTP headers')
parser.add_argument('--proxy', '-p', action='store', dest="proxy", default="http://localhost:8118", help='Use this proxy server (defaults to http://localhost:8118 - Tor)')
parser.add_argument('--no-proxy', '-n', action='store_true', dest="noproxy", default=False, help="Don't use a proxy server")
parser.add_argument('--redirects', '-r', action='store_true', dest="redirects", default=False, help='Display redirect URLs')
parser.add_argument('--user-agent', '-u', action='store', dest="useragent", default="Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.202 Safari/535.1", help='Send this User Agent')
parser.add_argument('--verbose', '-v', action='store_true', dest="verbose", default=False, help="Be verbose")
parser.add_argument('--no-content', '-z', action='store_true', dest="nocontent", default=False, help="Don't print body of HTML")
args = parser.parse_args()

# Set up some variables for ease of use - there's really no need for this but it makes my life easier
cookies = args.cookies
forms = args.forms
submitform = args.submitform
headers = args.headers
proxy = args.proxy
noproxy = args.noproxy
nocontent = args.nocontent
redirects = args.redirects
url = args.url
useragent = args.useragent
verbose = args.verbose

# Set up logging details
logger = logging.getLogger("mechanize")
logger.addHandler(logging.StreamHandler(sys.stderr))
logger.setLevel(logging.INFO)

# Set up the request
br = mechanize.Browser()
br.addheaders = [('User-agent', useragent)]
br.set_handle_robots(False)
br.set_handle_refresh(True)
br.set_handle_equiv(True)

# Set up cookie handling
COOKIEFILE = '/tmp/cookies.lwp'
cj = cookielib.LWPCookieJar()
br.set_cookiejar(cj)

# Do we want to log redirects?
if redirects is True:
	if verbose: print "Log redirects"
	br.set_debug_redirects(True)

# Set a proxy server unless noproxy
if noproxy is False:
	if verbose: print "Proxy set to %s" % proxy
	br.set_proxies({"http": proxy}) 

# Open the specified URL
try:
	r = br.open(url)
except IOError, e:
	print "%s: %s" % (url, e)
	sys.exit(1)
else:
	httpheaders = r.info()
	html = r.read()

	if forms:
		fc = 0
		print "Forms on page: "
		for f in br.forms():
			print "%d : %s : %s" % (fc, f.name, f)
			print f
			fc += 1

	if submitform is not None:
		if verbose: print "Submitting form %d" % submitform
		br.select_form(nr=submitform)
		br.submit()
		if verbose: print "Form response:"
		print br.response().read()
		

	if headers:
		print
		print httpheaders

	if cookies:
		print "Cookies received: "
		for index, cookie in enumerate(cj):
			print index, ' : ', cookie
		print
		#cj.save(COOKIEFILE)

	if nocontent is False:
		print html,
